# Results


## Model description and tuning
To evaluate different parameter values that are passed to the random forest, a tuning grid needs to be set up. The tuning grid serves the purpose of determining the best combination of parameters according to a range of predefined values for each of those parameters. The ranger implementation was used for this. The mtry parameter, refers to the number of variables that are randomly sampled at each split. In this case the mtry for the tuning grid was defined as mtry=c(2:9). The samp_size parameter specifies the number of samples to train on, spacifying a small number of samples may introduce bias and risk over-fitting. In this case the samp_size was defined as samp_size= c(.65, 0.7, 0.8, 0.9, 1). Finally the node_size parameter defines the complexity of the trees, it determines the minimum number of samples at terminal nodes. For the tuning grid node_size was set as node_size= seq(3,15,by=2). This is shown in the code below:

```{r eval=FALSE}
params <- expand.grid(
mtry= c(2:8),
node_size = seq(3, 15, by = 2), samp_size = c(.65, 0.7, 0.8, 0.9, 1)
)
```


Once the tuning grid has been defined, a loop that will pass each combination of parameters to the random forest algorithm, saving off the error measure:

```{r eval=FALSE}
for(i in 1:nrow(params)){
rf.i <- ranger(
 formula= reg.mod, data= train_x,
 num.trees= 5000,
 mtry= params$mtry[i], min.node.size= params$node_size[i],
 sample.fraction= params$samp_size[i],
 seed=123 
 )
# add OOB error to rf.grid
rf.grid <- c(rf.grid, sqrt(rf.i$prediction.error)) 
# print to see progress
if (i%%10 == 0) cat(i, "\t")
}
```

The best performing combination of parameters can be extracted using the which.min function. The best performing combination of parameters is mtry=3, node_size=7, samp_size=0.8, and can be applied to the final model. This is shown in the code below:

```{r eval=FALSE}
rfFit= ranger(
formula = reg.mod, 
data=train_x, 
num.trees = 5000, 
mtry = 3, 
min.node.size=7, 
sample.fraction =0.8, 
seed=123,
importance = "impurity"
)
```

According to the model, the three most important variables are accommodates, room_type_Private_room, and beds in descending order. This is shown in the figure below:

```{r echo=FALSE, fig.cap="Variable importance"}
varimp
```


## Random Forest Model Results
The Rsquared of the random forest model, is 0.55, suggesting that the final model explains about 55% of the variation. This is shown in the table below, along with the model’s other evaluation metrics.

```{r echo=FALSE}
RF_result<- tribble(
  ~RMSE, ~Rsquared, ~MAE,
  "0.49", "0.55", "0.36"
)
kable(RF_result)
```

In contrast, the OLS model’s Rsquared was 0.53, suggesting that the model explains about 53% of the variation. The model specifications are shown below:

```{r}
reg.mod =
  as.formula(log_price ~ accommodates + beds + bathrooms +
               cleaning_fee + property_type_House +
               property_type_Other + room_type_Private_room +
               room_type_Shared_room)
m = lm(reg.mod, data = data_anal)
summary(m)
```

The relative prices predicted by each model for the potential listings provided by the client are shown in the table below:

```{r echo=FALSE}
kable(final_prices)
```




