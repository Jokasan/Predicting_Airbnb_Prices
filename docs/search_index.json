[["index.html", "Predicting Airbnb listing prices using a Random Forest model Chapter 1 Executive Summary:", " Predicting Airbnb listing prices using a Random Forest model Nils Indreiten 2021-08-05 Chapter 1 Executive Summary: The scope of this study is to develop a random forest algorithm to predict the price of the client’s potential Airbnb listings. Section 1 introduces the business problem and the methods used to address it. Section 2, outlines details of the dataset and any pre-processing necessary, as well as presents the random forest model. Section 3 examines the random forests’ tuning process and presents the model results in comparison to an OLS model. Finally, Section 4 is a discussion, concluding with directions for future work. "],["intro.html", "Chapter 2 Introduction 2.1 Business problem and study aim 2.2 Study Aims and the Random Forest Method", " Chapter 2 Introduction 2.1 Business problem and study aim The client has presented a list of properties from their portfolio to list on Airbnb. The main issue is that the client needs an indication of the price at which the properties should be listed. In order to address this problem, a Random Forest model will be constructed, from training data, including a number of factors relating to the rental property (for example: # of bedrooms, how many people it accommodates, etc.). After the random forest model is constructed, it will then be applied to the data of potential listings provided by the client, to suggest adequate rental prices. (Breiman 1996) 2.2 Study Aims and the Random Forest Method 2.2.1 Study Aims The aim of this study is to suggest adequate prices to the client for their potential listings, using a Random Forest machine learning model. This study engages with the algorithmic modeling culture, whereby the goal is to find an algorithm f(x), such that for future x in a test set, f(x) will be a good predictor of y (Breiman 2001). More specifically, to find an algorithm that is the best predictor of rental prices for the client’s potential listings. 2.2.2 Random Forests Method Ensemble is a meta-learning approach, based on the premise that the combination of multiple weak learners, will yield a stronger learner (Lantz 2019). Random forests are an ensemble-based method, focusing on ensembles of decision trees (Lantz 2019). Breiman and Cutler were the main champions of this method. The Random Forests Method merges the base principles of bagging with random feature selection, adding additional diversity to decision tree models (Lantz 2019). Once ensembles of trees are developed (the forest), a vote is used to combine tree predictions. Random forests build upon regression trees and the CART approach developed by (L 1984). Random forests were proposed to overcome tree correlation that arises because of Bagging regression trees (Breiman 1996, 2001). The main advantage of Random Forests is that they attempt to reduce tree correlation. By introducing randomness to the tree development process, large collections of decorrelated trees can be built, and thus are less prone to overfitting. Another key advantage of random forests is that they only select the most important features. In addition, due to the ensemble only utilising a random fraction of the full feature set, random forests are adept at handling large datasets, overcoming the “curse of dimensionality” (Lantz 2019). On the other hand, a noticeable weakness that has been pointed out is that they are more difficult to interpret, than say a decision tree (Lantz 2019). Nevertheless, random forests are a versatile and powerful machine learning approach. References "],["methods.html", "Chapter 3 Methods 3.1 Data pre-processing 3.2 The Random Forest Model", " Chapter 3 Methods The data used to train the model in this study, originated from Inside Airbnb, an open access platform, that makes lettings data publicly available. Lettings data from Manchester in November 2019, was used for this study. The original data was very noisy and had to be cleaned, transformed and pre-processed, prior to training. 3.1 Data pre-processing The training data is made up of 4,846 observations and has 9 features. A list of the variables and their type is shown below, log_price being the target variable and the others the predictor variables. Given the messiness of the listings data, certain data pre-processing operations had to be done, in some cases some new variables had to be created. Variable Type Description log_price numeric The log transformation of the price from the raw data. Transformed due to a skewed distribution. accomodates integer No pre-processing was necessary for this variable. It is a measure of how many people the property accommodates. beds integer No pre-processing was necessary for this variable. It is a measure of how many people the property accommodates. bathrooms numeric No pre-processing was necessary for this variable. It is a measure of how many people the property accommodates. cleaning_fee numeric This variable was transformed to a binary value, 1 indicating the property has a cleaning fee and 0 no cleaning fee. property_type_House numeric This variable was transformed into a binary value, 1 indicating that the property is a house and 0 indicating it is not a House. property_type_Other numeric This variable was transformed into a binary value, 1 indicating that the property house is of type “other” and 0 indicating it is not other. room_type_Private_room numeric This variable was transformed into a binary variable, 1 indicating that the property’s room type is private and 0 indicating it is not private. room_type_Share_room numeric This variable was transformed into a binary variable, 1 indicating that the property’s room type is shared and 0 indicating it is not shared. The following data pre-processing was necessary to achieve the formatting of variables in the table above: ## The price variable: # Convert listing prices from dollars to numbers listings$price = dollar_to_number(listings$price) # Remove listings with price values of zeros: listings=listings[listings$price&gt;0,] ## The property_type and room_type variables: listings$property_type_House = (listings$property_type == &quot;House&quot;)+0 listings$property_type_Other = (listings$property_type == &quot;Other&quot;)+0 listings$room_type_Private_room = (listings$room_type == &quot;Private room&quot;)+0 listings$room_type_Shared_room = (listings$room_type == &quot;Shared room&quot;)+0 ## Filling in the data which is missing or NA, using the median value: listings$bathrooms[is.na(listings$bathrooms)]=median(listings$bathrooms, na.rm=T) listings$beds[is.na(listings$beds)]= median(listings$beds, na.rm=T) The first few entries of the pre-processed dataset are shown below: ## # A tibble: 6 x 9 ## log_price accommodates beds bathrooms cleaning_fee property_type_House ## &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 4.17 4 6 1 1 1 ## 2 4.09 2 1 1.5 0 0 ## 3 3.53 3 2 1 1 0 ## 4 4.01 2 2 1 0 1 ## 5 6.86 2 1 1 0 0 ## 6 4.09 2 1 1 0 1 ## # … with 3 more variables: property_type_Other &lt;dbl&gt;, ## # room_type_Private_room &lt;dbl&gt;, room_type_Shared_room &lt;dbl&gt; Furthermore, the correlations between the predictor variables can be visualised. There seems to be a high correlation between the beds and accommodates (0.804) variables, suggesting a colinearity, however given that the scope of this study is predictive modeling, this shouldn’t be an issue. Once the data was pre-processed and ready to be utilised, training and test datasets were specified, using the createDataPartition function, to ensure that the distribution of both sets, resembles the distribution of the whole dataset. # create training set partition using create data partition: training_index=createDataPartition(data_anal$log_price, p=0.7, list=F) train_x=data_anal[training_index,] test_x=data_anal[-training_index,] Distribution of Categorical variables: Distribution of continuous variables: 3.2 The Random Forest Model The regression model has been defined, with log_price as the target variable and the remaining 8 variables as predictors. The model is displayed below: reg.mod=log_price~ accommodates + beds + bathrooms + cleaning_fee + property_type_House + property_type_Other + room_type_Private_room + room_type_Shared_room An initial model can be created, using the RF implementation from the randomForest package. The model is set to run with 5,000 trees, to see if we get a model with the lowest error within this number. rf1 &lt;- randomForest( formula=reg.mod, ntree=5000, data=train_x ) "],["results.html", "Chapter 4 Results 4.1 Model description and tuning 4.2 Random Forest Model Results", " Chapter 4 Results 4.1 Model description and tuning To evaluate different parameter values that are passed to the random forest, a tuning grid needs to be set up. The tuning grid serves the purpose of determining the best combination of parameters according to a range of predefined values for each of those parameters. The ranger implementation was used for this. The mtry parameter, refers to the number of variables that are randomly sampled at each split. In this case the mtry for the tuning grid was defined as mtry=c(2:9). The samp_size parameter specifies the number of samples to train on, spacifying a small number of samples may introduce bias and risk over-fitting. In this case the samp_size was defined as samp_size= c(.65, 0.7, 0.8, 0.9, 1). Finally the node_size parameter defines the complexity of the trees, it determines the minimum number of samples at terminal nodes. For the tuning grid node_size was set as node_size= seq(3,15,by=2). This is shown in the code below: params &lt;- expand.grid( mtry= c(2:8), node_size = seq(3, 15, by = 2), samp_size = c(.65, 0.7, 0.8, 0.9, 1) ) Once the tuning grid has been defined, a loop that will pass each combination of parameters to the random forest algorithm, saving off the error measure: for(i in 1:nrow(params)){ rf.i &lt;- ranger( formula= reg.mod, data= train_x, num.trees= 5000, mtry= params$mtry[i], min.node.size= params$node_size[i], sample.fraction= params$samp_size[i], seed=123 ) # add OOB error to rf.grid rf.grid &lt;- c(rf.grid, sqrt(rf.i$prediction.error)) # print to see progress if (i%%10 == 0) cat(i, &quot;\\t&quot;) } The best performing combination of parameters can be extracted using the which.min function. The best performing combination of parameters is mtry=3, node_size=7, samp_size=0.8, and can be applied to the final model. This is shown in the code below: rfFit= ranger( formula = reg.mod, data=train_x, num.trees = 5000, mtry = 3, min.node.size=7, sample.fraction =0.8, seed=123, importance = &quot;impurity&quot; ) According to the model, the three most important variables are accommodates, room_type_Private_room, and beds in descending order. This is shown in the figure below: 4.2 Random Forest Model Results The Rsquared of the random forest model, is 0.55, suggesting that the final model explains about 55% of the variation. This is shown in the table below, along with the model’s other evaluation metrics. RMSE Rsquared MAE 0.49 0.55 0.36 In contrast, the OLS model’s Rsquared was 0.53, suggesting that the model explains about 53% of the variation. The model specifications are shown below: reg.mod = as.formula(log_price ~ accommodates + beds + bathrooms + cleaning_fee + property_type_House + property_type_Other + room_type_Private_room + room_type_Shared_room) m = lm(reg.mod, data = data_anal) summary(m) ## ## Call: ## lm(formula = reg.mod, data = data_anal) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0207 -0.3083 -0.0609 0.2217 5.0743 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.906290 0.025383 153.895 &lt; 2e-16 *** ## accommodates 0.114073 0.005815 19.616 &lt; 2e-16 *** ## beds 0.009721 0.007659 1.269 0.204405 ## bathrooms 0.069179 0.013832 5.001 5.89e-07 *** ## cleaning_fee -0.077549 0.016110 -4.814 1.53e-06 *** ## property_type_House -0.163889 0.018433 -8.891 &lt; 2e-16 *** ## property_type_Other 0.069984 0.020292 3.449 0.000568 *** ## room_type_Private_room -0.578816 0.019369 -29.883 &lt; 2e-16 *** ## room_type_Shared_room -0.748410 0.074809 -10.004 &lt; 2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5183 on 4837 degrees of freedom ## Multiple R-squared: 0.533, Adjusted R-squared: 0.5322 ## F-statistic: 689.9 on 8 and 4837 DF, p-value: &lt; 2.2e-16 The relative prices predicted by each model for the potential listings provided by the client are shown in the table below: ID OLS.Price RF.Price 1 $80 $80 2 $40 $35 3 $40 $35 4 $40 $35 5 $110 $95 6 $65 $70 7 $105 $100 8 $30 $30 9 $65 $70 10 $90 $100 "],["discussion.html", "Chapter 5 Discussion 5.1 Future directions", " Chapter 5 Discussion Relative to the OLS model’s Rsquared which explains around 53% of the variation, the RF model did improve, explaining 55% of the variation. However, there seems to be room for improvement, particularly to try and get closer to the arbitrary golden standard Rsquared measure of 70%. One improvement to the model, would be to include more features from the raw dataset. The random forest algorithm is naturally adept at handling large datasets, and therefore the number of features included in the model should be increased. For example, proximity to city centre could be a variable that can be developed from the raw data, and perhaps more importantly, have a significant influence on the listing’s potential price. Expanding the number of features included in the model, the tuning grid would have to be revised, particularly increasing the upper threshold of the mtry to accommodate for the number of predictors. With that being said, simply increasing the number of features is not a guarantee for increased model performance, and should therefore be approached with careful consideration. Breiman points out that, random forests don’t tend to overfit due to the Law of Large Numbers, because of the injection of randomness, they are accurate regressors (Breiman 2001). However, as Lantz pointed out, it is difficult to inspect the method through which random forests achieve their results (Breiman 2001). In fact, Breiman demonstrates that random features and inputs produce good results when confronted with a classification problem, as opposed to a regression problem. Similarly, from a theoretical point of view, the analysis of the model remains difficult, particularly when it comes to the dependencies between the induced partitions of the input space and predictions within the partitions (Lorenzen, Igel, and Y 2019). They suggest using PAC-Bayesian bounds, for the averaging and majority vote classifiers. This is mostly proposed, in order to overcome the deterioration of the C-bound, when there are strong individual classifiers and high correlation, which is the case with random forests; the bounds for the Gibbs classifier are more robust (Lorenzen, Igel, and Y 2019). The authors demonstrate that PAC-Bayesian generalisation bounds can be implemented to achieve robust performance guarantees, for the random forest classifier. The Gibbs classifier, outperformed the majority vote bounds, which take the correlation between ensemble members into consideration; the reason for this, according to the authors, is due to the high accuracy of decision trees as classifiers making estimation of correlations of error more difficult (Lorenzen, Igel, and Y 2019). In contrast, (Geurts, Ernst, and Wehenkel 2006) propose the extra-trees algorithm. What distinguishes the extra- trees algorithm from other tree-based ensemble methods, is that nodes are split by cut-points chosen fully at random and the tree growing process is done using the whole learning sample, as opposed to a boostrap replica, as does a random forest for instance. In other words, the novelty of this approach is the combination of the attribute randomisation of Random subspace along with a completely random selection of the cut-point (Geurts, Ernst, and Wehenkel 2006). 5.1 Future directions In conclusion, the random forest algorithm significantly improved the predictive performance from the OLS model. The next steps involve expanding the scope of the model to include other features present in the raw data and adjusting the tuning grid accordingly. Furthermore, the application of PAC-Bayesian bounds can be implemented into the model as well as building a model using the extra-trees algorithm, can be pursued to achieve a higher Rsquared measure. References "],["references.html", "References", " References "]]
